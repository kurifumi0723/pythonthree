# -*- coding: utf-8 -*-
"""game2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xks5NdXeTciUGMgn4oBML_i2NM5icvbZ
"""

# パッケージのインポート
import random
import math

#石の合計
PIECES_NUMBER = 9
#縦横のサイズ
WIDTH_NUMBER = 3
#連続のサイズ
WIN_NUMBER = 3

# ゲーム状態
class State:
    # 初期化
    def __init__(self, pieces=None, enemy_pieces=None):
        # 石の配置
        self.pieces = pieces if pieces != None else [0] * PIECES_NUMBER
        self.enemy_pieces = enemy_pieces if enemy_pieces != None else [0] * PIECES_NUMBER

    # 石の数の取得
    def piece_count(self, pieces):
        count = 0
        for i in pieces:
            if i == 1:
                 count += 1
        return count

    # 負けかどうか
    def is_lose(self):
        # 3並びかどうか
        def is_comp(x, y, dx, dy):
            for k in range(WIN_NUMBER):
                if y < 0 or WIDTH_NUMBER - 1 < y or x < 0 or WIDTH_NUMBER - 1 < x or self.enemy_pieces[x+y*WIDTH_NUMBER] == 0:
                     return False
                x, y = x+dx, y+dy
            return True
        # 負けかどうか
        if is_comp(0, 0, 1, 1) or is_comp(0, WIDTH_NUMBER - 1, 1, -1):
            return True
        for i in range(WIDTH_NUMBER):
            if is_comp(0, i, 1, 0) or is_comp(i, 0, 0, 1):
                return True
        return False

    # 引き分けかどうか
    def is_draw(self):
        return self.piece_count(self.pieces) + self.piece_count(self.enemy_pieces) == PIECES_NUMBER

    # ゲーム終了かどうか
    def is_done(self):
        return self.is_lose() or self.is_draw()

    # 次の状態の取得
    def next(self, action):
        pieces = self.pieces.copy()
        pieces[action] = 1
        return State(self.enemy_pieces ,pieces)

    # 合法手のリストの取得
    def legal_actions(self):
        actions = []
        for i in range(PIECES_NUMBER):
            if self.pieces[i] == 0 and self.enemy_pieces[i] == 0:
                actions.append(i)
        return actions

    # 先手かどうか
    def is_first_player(self):
        return self.piece_count(self.pieces) == self.piece_count(self.enemy_pieces)

    # 文字列の表示
    def __str__(self):
        ox = ('o', 'x') if self.is_first_player() else ('x', 'o')
        str = ''
        for i in range(PIECES_NUMBER):
            if self.pieces[i] == 1:
                str += ox[0]
            elif self.enemy_pieces[i] == 1:
                str += ox[1]
            else:
                str += '-'
            if i % WIDTH_NUMBER == WIDTH_NUMBER - 1:
                str += '\n'
        return str

# ランダムで行動選択
def random_action(state):
    legal_actions = state.legal_actions()
    return legal_actions[random.randint(0, len(legal_actions)-1)]

# アルファベータ法で状態価値計算
def alpha_beta(state, alpha, beta):
    # 負けは状態価値-1
    if state.is_lose():
        return -1

    # 引き分けは状態価値0
    if state.is_draw():
        return 0

    # 合法手の状態価値の計算
    alpha = -float('inf')
    for action in state.legal_actions():
        score = -alpha_beta(state.next(action), -beta, -alpha)
        if score > alpha:
           alpha = score

        # 現ノードのベストスコアが親ノードを超えたら探索終了
        if alpha >= beta:
            return alpha

    # 合法手の状態価値を返す
    return alpha

# アルファベータ法で行動選択
def alpha_beta_action(state):
    # 合法手の状態価値の計算
    best_action = 0
    alpha = -float('inf')
    str = ['','']
    for action in state.legal_actions():
        score = -alpha_beta(state.next(action), -float('inf'), -alpha)
        if score > alpha:
           best_action = action
           alpha = score

        str[0] = '{}{:2d},'.format(str[0], action)
        str[1] = '{}{:2d},'.format(str[1], score)
    print('action:', str[0], '\nscore', str[1], '\n')

    # 合法手の状態価値の最大値を持つ行動を返す
    return best_action
  
# プレイアウト
def playout(state):
    # 負けは状態価値-1
    if state.is_lose():
        return -1

    # 引き分けは状態価値0
    if state.is_draw():
        return 0

    # 次の状態価値を返す
    return -playout(state.next(random_action(state)))
        
# 最大値のインデックスを返す
def argmax(collection, key=None):
    return collection.index(max(collection))
    
# モンテカルロ木探索の行動選択
def mcts_action(state):
    # モンテカルロ木探索のノードの定義
    class Node:
         # 初期化
         def __init__(self, state):
             self.state = state # 状態
             self.w = 0 # 累計価値
             self.n = 0 # 試行価値
             self.child_nodes = None # 子ノード群

         # 局面の価値の計算
         def evaluate(self):
             # ゲーム終了時
             if self.state.is_done():
                 # 勝敗結果で価値を取得
                 value = -1 if self.state.is_lose() else 0 # 負けは-1、引き分けは0

                 # 累計価値と試行回数の更新
                 self.w += value
                 self.n += 1
                 return value
              
             # 子ノードが存在しない時
             if not self.child_nodes:
                 # プレイアウトで価値を取得
                 value = playout(self.state)

                 # 累計価値と試行回数の更新
                 self.w += value
                 self.n += 1

                 # 子ノードに展開
                 if self.n == 10:
                     self.expand()
                 return value

             # 子ノードが存在する場合
             else:
                 # UCB1が最大の子ノードの評価で価値を取得
                 value = -self.next_child_node().evaluate()

                 # 累計価値と試行回数の更新
                 self.w += value
                 self.n += 1
                 return value

         # 子ノードの展開
         def expand(self):
             legal_actions = self.state.legal_actions()
             self.child_nodes = []
             for action in legal_actions:
                 self.child_nodes.append(Node(self.state.next(action)))

         # UCB1が最大の子ノードの取得
         def next_child_node(self):
             # 試行回数が0の子ノードを返す
             for child_node in self.child_nodes:
                 if child_node.n == 0:
                     return child_node

             # UCB1の計算
             t = 0 
             for c in self.child_nodes:
                 t += c.n
             ucb1_values = []
             for child_node in self.child_nodes:
                 ucb1_values.append(-child_node.w/child_node.n+(2*math.log(t)/child_node.n)**0.5)

             # UCB1が最大の子ノードを返す
             return self.child_nodes[argmax(ucb1_values)]

    # 現在の局面のノード作成
    root_node = Node(state)
    root_node.expand()

    # 100回のシミュレーションを実行
    for _ in range(100):
        root_node.evaluate()

    # 試行回数の最大値を持つ行動を返す
    legal_actions = state.legal_actions()
    n_list = []
    for c in root_node.child_nodes:
        n_list.append(c.n)
    return legal_actions[argmax(n_list)]

#EP_GAME_COUNT = 100
#
#def first_player_point(ended_state):
#    if ended_state.is_lose():
#        return 0 if ended_state.is_first_player() else 1
#    return 0.5
#
#def play(next_actions):
#    state = State()
#    while True:
#        if state.is_done():
#            break
#        next_action = next_actions[0] if state.is_first_player() else next_actions[1]
#        action = next_action(state)
#        state = state.next(action)
#    return first_player_point(state)
#
#def evaluate_algorithm_of(label, next_actions):
#    total_point = 0
#    for i in range(EP_GAME_COUNT):
#        if i%2 == 0:
#            total_point += play(next_actions)
#        else:
#            total_point += 1 - play(list(reversed(next_actions)))
#        print('\rEvaluate{}/{}'.format(i+1, EP_GAME_COUNT) , end='')
#    print('')
#    average_point = total_point/EP_GAME_COUNT
#   print(label.format(average_point))

#next_actions = (mts_action,random_action)
#evaluate_algorithm_of('VS_Random{:.3f}',next_actions)

#next_actions = (mts_action,alpha_beta_action)
#evaluate_algorithm_of('VS_AlphaBeta{:.3f}',next_actions)

if __name__ == '__main__':
    state = State()
    while True:
       if state.is_done():
           break
       state = state.next(random_action(state))

       print(state)
       print()

#from google.colab import files
#uploaded = files.upload()
#!dir

#!python game2.py
#!rm game2.py